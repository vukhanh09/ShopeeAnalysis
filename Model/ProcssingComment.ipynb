{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# import torch\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "from vncorenlp import VnCoreNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "conf = SparkConf().setAppName(\"Process Comment\").setMaster(\"spark://25.15.27.228:7077\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Process Comment').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcssComment:\n",
    "    def __init__(self,sc,spark,type = 'Comment'):\n",
    "        self.sc = sc\n",
    "        self.type = type\n",
    "        self.spark = spark\n",
    "        self.URI           = self.sc._gateway.jvm.java.net.URI\n",
    "        self.Path          = self.sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "        self.FileSystem    = self.sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "        self.Configuration = self.sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base\").to(self.device)\n",
    "        # self.tokenizer_phobert = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "        self.rdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "        self.getList()\n",
    "        self.rates = ['1','2','3','4','5']\n",
    "        self.dictRate = {}\n",
    "\n",
    "    def getList(self):\n",
    "        fs = self.FileSystem.get(self.URI(\"hdfs://cris:9000\"), self.Configuration())\n",
    "        status = fs.listStatus(self.Path(f'/shopee/{self.type}'))\n",
    "        pathProducts = []\n",
    "        for fileStatus in status:\n",
    "            pathProducts.append(str(fileStatus.getPath()))\n",
    "        self.path = pathProducts[1:]\n",
    "    \n",
    "    def processComment(self):\n",
    "        for i in tqdm(range(len(self.path))):\n",
    "            url = self.path[i]\n",
    "\n",
    "            parDF2 = self.spark.read.parquet(url)\n",
    "            cols = ['comment','rating_star']\n",
    "            data = parDF2[cols].toPandas()\n",
    "            for rate in self.rates:\n",
    "                index = data[data['rating_star']==rate].index\n",
    "                if index.shape[0] !=0:\n",
    "                    if rate not in self.dictRate.keys():\n",
    "                        self.dictRate[rate] = data.loc[index,:]\n",
    "                    elif self.dictRate[rate].shape[0] < 50000:\n",
    "                        self.dictRate[rate] = pd.concat([self.dictRate[rate],data.loc[index,:]],axis = 0)\n",
    "        for rate in self.rates:\n",
    "            self.dictRate[rate] = self.dictRate[rate].reset_index(drop=True)\n",
    "\n",
    "            data = self.getFeature(self.dictRate[rate],'comment')\n",
    "            try:\n",
    "                self.uploadHdfs(data,rate)\n",
    "            except Exception as e:\n",
    "                print(f'Error when upload to HDFS {e}')\n",
    "\n",
    "    \n",
    "    def getFeature(self,data,col):\n",
    "        for x in tqdm(range(data.shape[0])):\n",
    "            sentences = self.rdrsegmenter.tokenize(data[col].iloc[x])\n",
    "            tmp = []\n",
    "            for sentence in sentences:\n",
    "                arr = \" \".join(sentence)\n",
    "                tmp.append(arr)\n",
    "            data.loc[x,col] = ' </s> <s> '.join(tmp)\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "    def uploadHdfs(self,data,rate):\n",
    "        data = spark.createDataFrame(data.astype(str))\n",
    "        data.coalesce(1).write.mode('append').parquet(f'hdfs://cris:9000/ProcessShopee/{self.type}/rate_{rate}')\n",
    "        print('Done upload HDFS')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = ProcssComment(sc,spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:38<00:00,  2.24s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21707/21707 [01:07<00:00, 322.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done upload HDFS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11046/11046 [00:26<00:00, 423.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done upload HDFS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27177/27177 [01:04<00:00, 424.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done upload HDFS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50046/50046 [01:58<00:00, 423.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done upload HDFS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74363/74363 [03:03<00:00, 405.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done upload HDFS\n"
     ]
    }
   ],
   "source": [
    "comment.processComment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done upload HDFS\n"
     ]
    }
   ],
   "source": [
    "rates = ['1','2','3','4','5']\n",
    "data = comment.dictRate['1']\n",
    "for rate in rates[1:]:\n",
    "    data = pd.concat([data,comment.dictRate[rate]],axis = 0)\n",
    "comment.uploadHdfs(data,'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done upload HDFS\n"
     ]
    }
   ],
   "source": [
    "rates = ['1','2','3','4','5']\n",
    "data = comment.dictRate['1']\n",
    "for rate in rates[1:]:\n",
    "    data = pd.concat([data,comment.dictRate[rate]],axis = 0)\n",
    "data = data.reset_index(drop=True)\n",
    "comment.uploadHdfs(data,'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('hdfs://cris:9000/ProcessShopee/Comment/rate_sum/part-00000-2bb5d99d-0093-4c2e-bc46-0b8487d2c0db-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rating_star'] = data['rating_star'].astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|             comment|rating_star|\n",
      "+--------------------+-----------+\n",
      "|Th·ª±c_s·ª± th·∫•t_v·ªçng...|          0|\n",
      "|H√†ng nh·∫≠n ƒëc kh√° ...|          0|\n",
      "|ƒë·∫∑t 1 ƒë√πi v√† 1 ng...|          0|\n",
      "|Ch·∫•t_li·ªáu qu√° t·ªìi...|          0|\n",
      "|Giao v·∫£i b·ªã l·ªói ....|          0|\n",
      "|M·ªèng te ko nh∆∞ m√¥...|          0|\n",
      "|Shop c·ªë_t√¨nh giao...|          0|\n",
      "|Shop ngo√†i c√°i ba...|          0|\n",
      "|S·∫£n_ph·∫©m k√©m ch·∫•t...|          0|\n",
      "|Mua √°o √°p_d·ª•ng m√£...|          0|\n",
      "| V·∫£i x·∫•u ƒëau_ƒë·ªõn ·∫° ,|          0|\n",
      "|        V·∫£i qu√° m·ªèng|          0|\n",
      "|Th·∫•t_v·ªçng t√¥i ƒë·ªët...|          0|\n",
      "|Mua 2 t·∫•m l∆∞·ªõi v·∫£...|          0|\n",
      "|Qu·∫ßn qu√° x·∫•u so v...|          0|\n",
      "|      ch·∫•t_l∆∞·ª£ng k√©m|          0|\n",
      "|Kh√¥ng t·ªët nh∆∞ qu·∫£...|          0|\n",
      "|Em ƒë·∫∑t 1 b·ªô m√† sh...|          0|\n",
      "|S·∫£n_ph·∫©m m√†u ko g...|          0|\n",
      "|R√µ_r√†ng kh√¥ng mua...|          0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.select(lower(col('comment')).alias('comment'), 'rating_star')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(split(dataset.comment, ' ').alias('comment'), 'rating_star')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=100, seed=42, inputCol=\"comment\", outputCol=\"feature\")\n",
    "word2Vec.setMaxIter(5)\n",
    "model = word2Vec.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|             comment|rating_star|             feature|\n",
      "+--------------------+-----------+--------------------+\n",
      "|[th·ª±c_s·ª±, th·∫•t_v·ªç...|          0|[-0.0390716940927...|\n",
      "|[h√†ng, nh·∫≠n, ƒëc, ...|          0|[-0.0165625282563...|\n",
      "|[ƒë·∫∑t, 1, ƒë√πi, v√†,...|          0|[0.05198483966971...|\n",
      "|[ch·∫•t_li·ªáu, qu√°, ...|          0|[-0.0672429638776...|\n",
      "|[giao, v·∫£i, b·ªã, l...|          0|[-0.0196016904382...|\n",
      "|[m·ªèng, te, ko, nh...|          0|[0.01527424864470...|\n",
      "|[shop, c·ªë_t√¨nh, g...|          0|[0.04810728304002...|\n",
      "|[shop, ngo√†i, c√°i...|          0|[-0.0524565114526...|\n",
      "|[s·∫£n_ph·∫©m, k√©m, c...|          0|[-0.0587866885600...|\n",
      "|[mua, √°o, √°p_d·ª•ng...|          0|[-0.0251391950296...|\n",
      "|[v·∫£i, x·∫•u, ƒëau_ƒë·ªõ...|          0|[-0.0165521422401...|\n",
      "|    [v·∫£i, qu√°, m·ªèng]|          0|[-0.2366287211577...|\n",
      "|[th·∫•t_v·ªçng, t√¥i, ...|          0|[-0.0203908173633...|\n",
      "|[mua, 2, t·∫•m, l∆∞·ªõ...|          0|[0.00853809400608...|\n",
      "|[qu·∫ßn, qu√°, x·∫•u, ...|          0|[-0.0190395279787...|\n",
      "|   [ch·∫•t_l∆∞·ª£ng, k√©m]|          0|[-0.1596387997269...|\n",
      "|[kh√¥ng, t·ªët, nh∆∞,...|          0|[-0.0358816581591...|\n",
      "|[em, ƒë·∫∑t, 1, b·ªô, ...|          0|[0.14215989050765...|\n",
      "|[s·∫£n_ph·∫©m, m√†u, k...|          0|[-0.0616485516452...|\n",
      "|[r√µ_r√†ng, kh√¥ng, ...|          0|[-0.0755672243734...|\n",
      "+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = model.transform(dataset)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = res.select('feature', 'rating_star')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.withColumnRenamed('feature', 'features')\n",
    "data1 = data1.withColumnRenamed('rating_star', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "splits = data1.randomSplit([0.6, 0.4], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-0.9046872377395...|    0|\n",
      "|[-0.8961412668228...|    1|\n",
      "|[-0.7522539639224...|    0|\n",
      "|[-0.6465824196736...|    0|\n",
      "|[-0.6002849489450...|    0|\n",
      "|[-0.5608168616890...|    0|\n",
      "|[-0.5276502370834...|    0|\n",
      "|[-0.5199735164642...|    0|\n",
      "|[-0.5196515843272...|    0|\n",
      "|[-0.5084541020914...|    0|\n",
      "|[-0.4971521297203...|    0|\n",
      "|[-0.4914908634223...|    0|\n",
      "|[-0.4811887666583...|    0|\n",
      "|[-0.4713759645819...|    0|\n",
      "|[-0.4700897104210...|    0|\n",
      "|[-0.4645945083828...|    0|\n",
      "|[-0.4563015364110...|    1|\n",
      "|[-0.4452980682253...|    0|\n",
      "|[-0.4372306317090...|    0|\n",
      "|[-0.4172227638463...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = splits[0]\n",
    "testData = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|             4|    1|[-0.8577913641929...|\n",
      "|             4|    0|[-0.5640047788619...|\n",
      "|             4|    0|[-0.3846752941608...|\n",
      "|             3|    0|[-0.3684847354888...|\n",
      "|             3|    1|[-0.3684847354888...|\n",
      "|             4|    0|[-0.3640398979187...|\n",
      "|             4|    0|[-0.3337194621562...|\n",
      "|             4|    0|[-0.3337194621562...|\n",
      "|             4|    0|[-0.3311986215412...|\n",
      "|             4|    0|[-0.3116985559463...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test Error = 0.49441\n",
      "RandomForestClassificationModel: uid=RandomForestClassifier_a9e1fb7a90e1, numTrees=50, numClasses=5, numFeatures=100\n"
     ]
    }
   ],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=50)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    22326\n",
       "3    14997\n",
       "2     8197\n",
       "0     6556\n",
       "1     3295\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.toPandas().label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    52037\n",
       "3    35049\n",
       "2    18980\n",
       "0    15151\n",
       "1     7751\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.toPandas().label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.5911463629584435\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# splits = data1.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [50, 60, 60, 5]\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=500, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(train)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec_5c0b34e806c0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = (\"a b \" * 100 + \"a c \" * 10).split(\" \")\n",
    "doc = spark.createDataFrame([(sent,), (sent,)], [\"sentence\"])\n",
    "word2Vec = Word2Vec(vectorSize=5, seed=42, inputCol=\"sentence\", outputCol=\"model\")\n",
    "word2Vec.setMaxIter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            sentence|\n",
      "+--------------------+\n",
      "|[a, b, a, b, a, b...|\n",
      "|[a, b, a, b, a, b...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Vec.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Vec.clear(word2Vec.maxIter)\n",
    "model = word2Vec.fit(doc)\n",
    "model.getMinCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dqp/.local/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|word|              vector|\n",
      "+----+--------------------+\n",
      "|   a|[0.09511695802211...|\n",
      "|   b|[-1.2028766870498...|\n",
      "|   c|[0.30153274536132...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.setInputCol(\"sentence\")\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[a, b, a, b, a, b, a, b, a, b, a, b, a, b, a, ...</td>\n",
       "      <td>[-0.4833007957870605, 0.18547806319911286, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[a, b, a, b, a, b, a, b, a, b, a, b, a, b, a, ...</td>\n",
       "      <td>[-0.4833007957870605, 0.18547806319911286, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [a, b, a, b, a, b, a, b, a, b, a, b, a, b, a, ...   \n",
       "1  [a, b, a, b, a, b, a, b, a, b, a, b, a, b, a, ...   \n",
       "\n",
       "                                               model  \n",
       "0  [-0.4833007957870605, 0.18547806319911286, -0....  \n",
       "1  [-0.4833007957870605, 0.18547806319911286, -0....  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.4833, 0.1855, -0.273, -0.0509, -0.4769])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.iloc[0].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dqp/.local/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t = model.getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = t.select('vector').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0951, 0.3911, -0.43, -0.1411, -0.0656])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.vector.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating_star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Th·ª±c_s·ª± th·∫•t_v·ªçng v·ªÅ s·∫£n_ph·∫©m . &lt;/s&gt; &lt;s&gt; Giao ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H√†ng nh·∫≠n ƒëc kh√° l√¢u ch·∫Øc do d·ªãch . &lt;/s&gt; &lt;s&gt; H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ƒë·∫∑t 1 ƒë√πi v√† 1 ng·ªë size M th√¨ g·ª≠i 2 ng·ªë 2 size...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ch·∫•t_li·ªáu qu√° t·ªìi , tr∆°n v√† b√≥ng y_nh∆∞ cao_su ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giao v·∫£i b·ªã l·ªói . 1 ƒë∆∞·ªùng d√†i 40cm . &lt;/s&gt; &lt;s&gt; ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184334</th>\n",
       "      <td>·ªêp xink , ch·∫Øc_ch·∫Øn ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184335</th>\n",
       "      <td>Xinh l·∫Øm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184336</th>\n",
       "      <td>·ªëp ƒë·∫πp qu√°_tr·ªùi m·ªçi ng·ª´i n√™nmua nha v√¨ l√† ƒë·ªì t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184337</th>\n",
       "      <td>ƒê·∫πp l·∫Øm , c·∫ßm_ch·∫Øc tay , khum ph√≠ t√¨nnn Nma gi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184338</th>\n",
       "      <td>Bshshxbajajxbahahsshshshsjsjzjyxysvxgxhxhdhshs...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184339 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment rating_star\n",
       "0       Th·ª±c_s·ª± th·∫•t_v·ªçng v·ªÅ s·∫£n_ph·∫©m . </s> <s> Giao ...           1\n",
       "1       H√†ng nh·∫≠n ƒëc kh√° l√¢u ch·∫Øc do d·ªãch . </s> <s> H...           1\n",
       "2       ƒë·∫∑t 1 ƒë√πi v√† 1 ng·ªë size M th√¨ g·ª≠i 2 ng·ªë 2 size...           1\n",
       "3       Ch·∫•t_li·ªáu qu√° t·ªìi , tr∆°n v√† b√≥ng y_nh∆∞ cao_su ...           1\n",
       "4       Giao v·∫£i b·ªã l·ªói . 1 ƒë∆∞·ªùng d√†i 40cm . </s> <s> ...           1\n",
       "...                                                   ...         ...\n",
       "184334  ·ªêp xink , ch·∫Øc_ch·∫Øn ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞ü•∞...           5\n",
       "184335                                           Xinh l·∫Øm           5\n",
       "184336  ·ªëp ƒë·∫πp qu√°_tr·ªùi m·ªçi ng·ª´i n√™nmua nha v√¨ l√† ƒë·ªì t...           5\n",
       "184337  ƒê·∫πp l·∫Øm , c·∫ßm_ch·∫Øc tay , khum ph√≠ t√¨nnn Nma gi...           5\n",
       "184338  Bshshxbajajxbahahsshshshsjsjzjyxysvxgxhxhdhshs...           5\n",
       "\n",
       "[184339 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0c47fc68ffbbf60d61484e788c5fda40d927438cafb86813efe033e797683a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
